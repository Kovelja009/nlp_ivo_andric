{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Serbian language model\n",
    "nlp = spacy.load('hr_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_letters(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all letters from the specified folder\n",
    "    Returns a DataFrame with filename and content\n",
    "    \"\"\"\n",
    "    letters = []\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                letters.append({\n",
    "                    'filename': filename,\n",
    "                    'content': content\n",
    "                })\n",
    "\n",
    "    print(f\"Loaded {len(letters)} letters.\")\n",
    "    \n",
    "    return pd.DataFrame(letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 letters.\n"
     ]
    }
   ],
   "source": [
    "letters_df = read_letters(\"letters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "Splitting the letters into sentences and performing the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentences(df):\n",
    "    \"\"\"\n",
    "    Analyzes sentences in each letter, focusing on length and complexity\n",
    "    Returns DataFrame with detailed sentence analysis\n",
    "    \"\"\"\n",
    "    sentence_analysis = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        doc = nlp(row['content'])\n",
    "        \n",
    "        # Analyze each sentence\n",
    "        for sent in doc.sents:\n",
    "            # Count words (excluding punctuation)\n",
    "            word_count = len([token for token in sent if not token.is_punct])\n",
    "            \n",
    "            # Calculate average word length (excluding punctuation)\n",
    "            avg_word_length = sum(len(token.text) for token in sent if not token.is_punct) / word_count if word_count > 0 else 0\n",
    "            \n",
    "            sentence_analysis.append({\n",
    "                'filename': row['filename'],\n",
    "                'sentence': sent.text,\n",
    "                'word_count': word_count,\n",
    "                'avg_word_length': round(avg_word_length, 2),\n",
    "                'is_long': word_count > 20  # Flag for long sentences\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(sentence_analysis)\n",
    "\n",
    "def get_sentence_statistics(sentence_df):\n",
    "    \"\"\"\n",
    "    Calculates statistical summary of sentence analysis\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'total_sentences': len(sentence_df),\n",
    "        'avg_sentence_length': sentence_df['word_count'].mean(),\n",
    "        'long_sentences_count': len(sentence_df[sentence_df['is_long']]),\n",
    "        'max_sentence_length': sentence_df['word_count'].max(),\n",
    "        'min_sentence_length': sentence_df['word_count'].min()\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "\n",
    "def analyze_letters_sentences(letters_df):\n",
    "    \"\"\"\n",
    "    Main function to analyze all letters and their sentences\n",
    "    \"\"\"\n",
    "    # Analyze sentences\n",
    "    sentence_analysis = analyze_sentences(letters_df)\n",
    "    \n",
    "    # Get statistics\n",
    "    stats = get_sentence_statistics(sentence_analysis)\n",
    "    \n",
    "    # Get notable sentences (longer than 20 words)\n",
    "    long_sentences = sentence_analysis[sentence_analysis['is_long']].sort_values('word_count', ascending=False)\n",
    "    \n",
    "    return letters_df, sentence_analysis, stats, long_sentences\n",
    "\n",
    "def print_analysis_summary(stats, long_sentences):\n",
    "    print(\"\\nSentence Analysis in Andrić's Letters:\")\n",
    "    print(\"=====================================\")\n",
    "    print(f\"Total number of analyzed sentences: {stats['total_sentences']}\")\n",
    "    print(f\"Average sentence length: {stats['avg_sentence_length']:.2f} words\")\n",
    "    print(f\"Number of long sentences (>20 words): {stats['long_sentences_count']}\")\n",
    "    print(f\"Longest sentence has {stats['max_sentence_length']} words\")\n",
    "    print(\"=====================================\")\n",
    "    \n",
    "    len_long_sentences = stats['long_sentences_count']\n",
    "    # check whether long sentences is not empty\n",
    "    if len_long_sentences != 0:\n",
    "        # print first min between len of long_sentences and 3\n",
    "        print(\"\\nExamples of the longest sentences:\")\n",
    "        print(\"=====================================\")\n",
    "\n",
    "        for _, row in long_sentences.head(min(len_long_sentences, 7)).iterrows():\n",
    "            print(f\"Letter: {row['filename']}\")\n",
    "            print(f\"Sentence: {row['sentence']}\")\n",
    "            print(f\"Word count: {row['word_count']}\")\n",
    "            print(f\"Avg. word length: {row['avg_word_length']}\")\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andrić writes relatively short sentences, there are only around 15% of sentences that have more than 20 words. The average sentence length is 12 words. His writting style is simplistic and easy to read, there are not many unnecessary adjectives in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence Analysis in Andrić's Letters:\n",
      "=====================================\n",
      "Total number of analyzed sentences: 82\n",
      "Average sentence length: 11.72 words\n",
      "Number of long sentences (>20 words): 12\n",
      "Longest sentence has 50 words\n",
      "=====================================\n",
      "\n",
      "Examples of the longest sentences:\n",
      "=====================================\n",
      "Letter: letter_2.txt\n",
      "Sentence: U tome, kako treba dočekati bolest i podnositi Ьоl,\n",
      "ја bih mogao biti Vaš učenik i primati Vaše savete а ne\n",
      "Vi moje.\n",
      "Sad, kad је sve па dobrom putu, nadam se da ćemo\n",
      "se još ovoga leta videti i porazgovarati, i radujem se\n",
      "unapred.\n",
      "Word count: 50\n",
      "Avg. word length: 3.66\n",
      "\n",
      "Letter: letter_2.txt\n",
      "Sentence: Gledaću svakako da dođem па jedan dan u\n",
      "Zagreb, kad budem polazio па odmor; а dotle, držite se\n",
      "dobro i budite tvrdi kao uvek, dragi, stari mој Tugomire.\n",
      "Grli Vas i pozdravlja\n",
      "Vaš\n",
      "Ivo\n",
      "\n",
      "Word count: 40\n",
      "Avg. word length: 3.73\n",
      "\n",
      "Letter: letter_5.txt\n",
      "Sentence: ,\n",
      "šaljem vam u prilogu 50 franaka i molim vas da\n",
      "budete ljubazni i da mi pošaljete Antologiju Lirike \"Misli\"\n",
      "(poslednje izdanje, sa slikаmа) i Antologiju Ljubavnе Lirike \n",
      "od g. Bož. Kovačevića.\n",
      "\n",
      "Word count: 36\n",
      "Avg. word length: 4.39\n",
      "\n",
      "Letter: letter_2.txt\n",
      "Sentence: Ра ipak hocu da vam se javim sa dve\n",
      "reči, da Vam kažem ono što dobro znate, koliko učestvujem\n",
      "u Vašoj bolesti i kako Vam od srca želim\n",
      "zdravlje i svako dobro.\n",
      "Word count: 35\n",
      "Avg. word length: 3.63\n",
      "\n",
      "Letter: letter_11.txt\n",
      "Sentence: - Čitao sam\n",
      "za imenovanje g. tate, znam da је zaposlen ра vas\n",
      "molim da mu vi izručite mоје pozdrave i čestitke; često\n",
      "sam u duhu s njim.\n",
      "Word count: 30\n",
      "Avg. word length: 3.6\n",
      "\n",
      "Letter: letter_1.txt\n",
      "Sentence: Žaleći što nisam mogao da vas јoš jednom vidim\n",
      "pozdravlja srdačno gospođu i Vas\n",
      "Uvijek Vaš\n",
      "Ivo\n",
      "Molim vas pozdravite Веbu i Radicu\n",
      "\n",
      "\n",
      "Word count: 28\n",
      "Avg. word length: 4.04\n",
      "\n",
      "Letter: letter_4.txt\n",
      "Sentence: У току ове недеље ћу Вам послати\n",
      "новац преко Марибора, а данас Вам шаљем позаjмљену кљигу,\n",
      "да је, са поздравима и благодарношћу,\n",
      "вратите г.\n",
      "Word count: 27\n",
      "Avg. word length: 4.22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "letters_df, sentence_analysis, stats, long_sentences = analyze_letters_sentences(letters_df)\n",
    "sentences = sentence_analysis['sentence']\n",
    "print_analysis_summary(stats, long_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:\n",
    "Tokenizing the words and removing the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize the input sentences\n",
    "    \n",
    "    Parameters:\n",
    "    sentences (list): List of sentences to tokenize\n",
    "    \n",
    "    Returns:\n",
    "    list: List of lists where each inner list contains tokens for one sentence\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        doc = nlp(sentence)\n",
    "        # Get all tokens that aren't punctuation or whitespace\n",
    "        tokens = [token.text.lower() for token in doc \n",
    "                if not token.is_punct and not token.is_space]\n",
    "        tokenized_sentences.append(tokens)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "def remove_stop_words(sentences):\n",
    "    \"\"\"\n",
    "    Remove stop words from tokenized sentences\n",
    "    \n",
    "    Parameters:\n",
    "    sentences (list): List of lists containing tokenized sentences\n",
    "    \n",
    "    Returns:\n",
    "    list: List of lists with stop words removed\n",
    "    \"\"\"\n",
    "    cleaned_sentences = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Process each token to check if it's a stop word\n",
    "        cleaned_tokens = [token for token in sentence \n",
    "                        if not nlp.vocab[token].is_stop]\n",
    "        cleaned_sentences.append(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: \n",
      "\n",
      "['ја', 'sam', 'proveo', 'vrlo', 'rđavu', 'zimu', 'јеr', 'sam', 'dva', 'puta', 'рrеbоlео', 'grip', 'i', 'sada', 'vučem', 'posledice']\n",
      "##############################################\n",
      "After removing stop words: \n",
      "\n",
      "['ја', 'proveo', 'rđavu', 'zimu', 'јеr', 'dva', 'puta', 'рrеbоlео', 'grip', 'sada', 'vučem', 'posledice']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = tokenize(sentences)\n",
    "cleaned_sentences = remove_stop_words(tokenized_sentences)\n",
    "print(\"Tokenized sentence: \\n\")\n",
    "print(tokenized_sentences[3])\n",
    "print(\"##############################################\")\n",
    "print(\"After removing stop words: \\n\")\n",
    "print(cleaned_sentences[3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
